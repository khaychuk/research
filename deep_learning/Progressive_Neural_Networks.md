# Progressive Neural Networks

Basic idea: have a neural net on one task, then for new tasks, "laterally
expand" by creating "new columns" in the architecture.

I can see why this works, but obviously has the issue of parameter scalability
--- let's see how well it works on other tasks. I need to read the papers that
cite it.

I'm surprised DeepMind didn't try and get this accepted to a conference.

TODO: more detailed notes in progress ...
