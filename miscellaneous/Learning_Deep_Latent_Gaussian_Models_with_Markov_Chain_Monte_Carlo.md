# Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo

**Main idea**: most deep latent Gaussian models are trained using variational
inference, but the author proposes an alternative using MCMC. This is slightly
more expensive, but brings benefits such as sharper images and higher likelihood
scores. ([See our BAIR Blog post][1] where we discuss the variational vs MCMC
approaches in a paragraph.)

Just to be clear, here's how the author defines a "Deep Latent Gaussian Model"
(DLGM):

> These models assume that the observed data were generated by sampling some
> latent variables, feeding them into a deep neural network, and adding some
> [Gaussian] noise to that network's output.

VAEs are an *example* of a DLGM, and arguably the "default" choice.

Remember, we're in an optimization setting and trying to fit a generative model
(with *latent variables*) to data.


**How does it work?**

- Use HMC, but instead of thinking of position variables as "theta" or the
  model parameters, think of those as *latent variables* `z`. The data `x` is
  not sampled, it is fixed and included in `p(x,z)` which we take gradients (of
  the log, of course).

- Since HMC (and MCMC more generally) takes a long time to reach the posterior
  (the "burn-in") he actually initializes the sampling with variational methods,
  and *then* applies HMC to improve upon that initialization. It reminds me of
  how Generative Adversarial Imitation Learning can be used on top of the
  simpler Behavior Cloning so that the Generator starts in a "good spot."

- Refinement 1: learning a *shared shearing matrix*.

- Refinement 2: setting step sizes and number of leapfrog steps. The sizes are
  set adaptively to aim for a given acceptance, which is in fact the way I like
  to do things. The leapfrog step count is set according to a heuristic based on
  computing periods of a sinusoid. There's a little catch based on adaptive MCMC
  sampling, though; let me check this out. Also, note that his adaptation is
  only on the worst-case scenario, not on the average-case ...

Could I implement this? Not right now unfortunately ... there are still too many
moving parts, and I need to work on implementing variational methods first.


**Experiments**:

- All done on "dynamically binarized permutation-invariant" MNIST.

- Held-out likelihood estimated using *Annealed Importance Sampling* (AIS).

- He argues that using MCMC methods eliminates over-pruning and blurriness with
  VAEs. Indeed, blurring is a known issue with VAEs compared to GANs. Pruning,
  I'm not totally sure about the *visual* consequence ... there's a math
  definition in the paper, though.

I'd like to try this out if I can find the time. I personally am not that
convinced that there's a tremendous quality difference inherent in these
algorithms (seems like hyperparameters and other stuff  would be much more
important) but I might be convinced otherwise.

Also on my TODO list: *understanding ELBO completely*.

[1]:http://bair.berkeley.edu/blog/2017/08/02/minibatch-metropolis-hastings/
